2025-03-07 03:02:55,682 - root - INFO - {'seed': 2023, 'model_dir': '../model', 'checkpoint_dir': '../checkpoint', 'model_name': 'model.pt', 'log_dir': '../log', 'distributed': 1, 'gpu': '2,3', 'master_addr': 'localhost', 'master_port': '1984', 'logging_level': 20, 'data_path': '../data', 'item_indexing': 'sequential', 'tasks': 'sequential,straightforward', 'datasets': 'ML1M', 'prompt_file': '../prompt.txt', 'sequential_order': 'original', 'collaborative_token_size': 200, 'collaborative_cluster': 20, 'collaborative_last_token': 'sequential', 'collaborative_float32': 0, 'max_his': 20, 'his_prefix': 1, 'his_sep': ' , ', 'skip_empty_his': 1, 'valid_prompt': 'seen:0', 'valid_prompt_sample': 1, 'valid_sample_num': '3,3', 'test_prompt': 'seen:0', 'sample_prompt': 1, 'sample_num': '3,3', 'batch_size': 128, 'eval_batch_size': 20, 'dist_sampler': 0, 'optim': 'AdamW', 'epochs': 10, 'lr': 0.001, 'clip': 1, 'logging_step': 100, 'warmup_prop': 0.05, 'gradient_accumulation_steps': 1, 'weight_decay': 0.01, 'adam_eps': 1e-06, 'dropout': 0.1, 'alpha': 2, 'train': 1, 'backbone': 't5-small', 'metrics': 'hit@5,hit@10,ndcg@5,ndcg@10', 'load': 0, 'random_initialize': 1, 'test_epoch': 0, 'valid_select': 0, 'test_before_train': 0, 'test_filtered': 0, 'test_filtered_batch': 1, 'world_size': 2, 'rank': 0, 'log_name': '1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt', 'model_path': '../model/ML1M/1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt.pt'}
2025-03-07 03:02:55,684 - root - INFO - Generating data for ML1M dataset
2025-03-07 03:02:55,684 - root - INFO - Get prompt template from ../prompt.txt
2025-03-07 03:11:11,810 - root - INFO - {'seed': 2023, 'model_dir': '../model', 'checkpoint_dir': '../checkpoint', 'model_name': 'model.pt', 'log_dir': '../log', 'distributed': 1, 'gpu': '2,3', 'master_addr': 'localhost', 'master_port': '1984', 'logging_level': 20, 'data_path': '../data', 'item_indexing': 'sequential', 'tasks': 'sequential,straightforward', 'datasets': 'ML1M', 'prompt_file': '../prompt.txt', 'sequential_order': 'original', 'collaborative_token_size': 200, 'collaborative_cluster': 20, 'collaborative_last_token': 'sequential', 'collaborative_float32': 0, 'max_his': 20, 'his_prefix': 1, 'his_sep': ' , ', 'skip_empty_his': 1, 'valid_prompt': 'seen:0', 'valid_prompt_sample': 1, 'valid_sample_num': '3,3', 'test_prompt': 'seen:0', 'sample_prompt': 1, 'sample_num': '3,3', 'batch_size': 128, 'eval_batch_size': 20, 'dist_sampler': 0, 'optim': 'AdamW', 'epochs': 10, 'lr': 0.001, 'clip': 1, 'logging_step': 100, 'warmup_prop': 0.05, 'gradient_accumulation_steps': 1, 'weight_decay': 0.01, 'adam_eps': 1e-06, 'dropout': 0.1, 'alpha': 2, 'train': 1, 'backbone': 't5-small', 'metrics': 'hit@5,hit@10,ndcg@5,ndcg@10', 'load': 0, 'random_initialize': 1, 'test_epoch': 0, 'valid_select': 0, 'test_before_train': 0, 'test_filtered': 0, 'test_filtered_batch': 1, 'world_size': 2, 'rank': 0, 'log_name': '1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt', 'model_path': '../model/ML1M/1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt.pt'}
2025-03-07 03:11:11,811 - root - INFO - Generating data for ML1M dataset
2025-03-07 03:11:11,811 - root - INFO - Get prompt template from ../prompt.txt
2025-03-07 03:12:38,033 - root - INFO - {'seed': 2023, 'model_dir': '../model', 'checkpoint_dir': '../checkpoint', 'model_name': 'model.pt', 'log_dir': '../log', 'distributed': 1, 'gpu': '2,3', 'master_addr': 'localhost', 'master_port': '1984', 'logging_level': 20, 'data_path': '../data', 'item_indexing': 'sequential', 'tasks': 'sequential,straightforward', 'datasets': 'ML1M', 'prompt_file': '../prompt.txt', 'sequential_order': 'original', 'collaborative_token_size': 200, 'collaborative_cluster': 20, 'collaborative_last_token': 'sequential', 'collaborative_float32': 0, 'max_his': 20, 'his_prefix': 1, 'his_sep': ' , ', 'skip_empty_his': 1, 'valid_prompt': 'seen:0', 'valid_prompt_sample': 1, 'valid_sample_num': '3,3', 'test_prompt': 'seen:0', 'sample_prompt': 1, 'sample_num': '3,3', 'batch_size': 128, 'eval_batch_size': 20, 'dist_sampler': 0, 'optim': 'AdamW', 'epochs': 10, 'lr': 0.001, 'clip': 1, 'logging_step': 100, 'warmup_prop': 0.05, 'gradient_accumulation_steps': 1, 'weight_decay': 0.01, 'adam_eps': 1e-06, 'dropout': 0.1, 'alpha': 2, 'train': 1, 'backbone': 't5-small', 'metrics': 'hit@5,hit@10,ndcg@5,ndcg@10', 'load': 0, 'random_initialize': 1, 'test_epoch': 0, 'valid_select': 0, 'test_before_train': 0, 'test_filtered': 0, 'test_filtered_batch': 1, 'world_size': 2, 'rank': 0, 'log_name': '1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt', 'model_path': '../model/ML1M/1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt.pt'}
2025-03-07 03:12:38,033 - root - INFO - Generating data for ML1M dataset
2025-03-07 03:12:38,033 - root - INFO - Get prompt template from ../prompt.txt
2025-03-07 03:12:38,039 - root - INFO - Considering {dataset} user_{user_id} has interacted with {dataset} items {history} . What is the next recommendation for the user ?
2025-03-07 03:12:38,039 - root - INFO - Required info: ['history', 'user_id', 'target', 'dataset']
2025-03-07 03:13:36,142 - root - INFO - {'seed': 2023, 'model_dir': '../model', 'checkpoint_dir': '../checkpoint', 'model_name': 'model.pt', 'log_dir': '../log', 'distributed': 1, 'gpu': '2,3', 'master_addr': 'localhost', 'master_port': '1984', 'logging_level': 20, 'data_path': '../data', 'item_indexing': 'sequential', 'tasks': 'sequential,straightforward', 'datasets': 'ML1M', 'prompt_file': '../prompt.txt', 'sequential_order': 'original', 'collaborative_token_size': 200, 'collaborative_cluster': 20, 'collaborative_last_token': 'sequential', 'collaborative_float32': 0, 'max_his': 20, 'his_prefix': 1, 'his_sep': ' , ', 'skip_empty_his': 1, 'valid_prompt': 'seen:0', 'valid_prompt_sample': 1, 'valid_sample_num': '3,3', 'test_prompt': 'seen:0', 'sample_prompt': 1, 'sample_num': '3,3', 'batch_size': 128, 'eval_batch_size': 20, 'dist_sampler': 0, 'optim': 'AdamW', 'epochs': 10, 'lr': 0.001, 'clip': 1, 'logging_step': 100, 'warmup_prop': 0.05, 'gradient_accumulation_steps': 1, 'weight_decay': 0.01, 'adam_eps': 1e-06, 'dropout': 0.1, 'alpha': 2, 'train': 1, 'backbone': 't5-small', 'metrics': 'hit@5,hit@10,ndcg@5,ndcg@10', 'load': 0, 'random_initialize': 1, 'test_epoch': 0, 'valid_select': 0, 'test_before_train': 0, 'test_filtered': 0, 'test_filtered_batch': 1, 'world_size': 2, 'rank': 0, 'log_name': '1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt', 'model_path': '../model/ML1M/1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt.pt'}
2025-03-07 03:13:36,142 - root - INFO - Generating data for ML1M dataset
2025-03-07 03:13:36,143 - root - INFO - Get prompt template from ../prompt.txt
2025-03-07 03:13:36,144 - root - INFO - Considering {dataset} user_{user_id} has interacted with {dataset} items {history} . What is the next recommendation for the user ?
2025-03-07 03:13:36,144 - root - INFO - Required info: ['user_id', 'target', 'history', 'dataset']
2025-03-07 03:17:04,711 - root - INFO - {'seed': 2023, 'model_dir': '../model', 'checkpoint_dir': '../checkpoint', 'model_name': 'model.pt', 'log_dir': '../log', 'distributed': 1, 'gpu': '2,3', 'master_addr': 'localhost', 'master_port': '1984', 'logging_level': 20, 'data_path': '../data', 'item_indexing': 'sequential', 'tasks': 'sequential,straightforward', 'datasets': 'ML1M', 'prompt_file': '../prompt.txt', 'sequential_order': 'original', 'collaborative_token_size': 200, 'collaborative_cluster': 20, 'collaborative_last_token': 'sequential', 'collaborative_float32': 0, 'max_his': 20, 'his_prefix': 1, 'his_sep': ' , ', 'skip_empty_his': 1, 'valid_prompt': 'seen:0', 'valid_prompt_sample': 1, 'valid_sample_num': '3,3', 'test_prompt': 'seen:0', 'sample_prompt': 1, 'sample_num': '3,3', 'batch_size': 128, 'eval_batch_size': 20, 'dist_sampler': 0, 'optim': 'AdamW', 'epochs': 10, 'lr': 0.001, 'clip': 1, 'logging_step': 100, 'warmup_prop': 0.05, 'gradient_accumulation_steps': 1, 'weight_decay': 0.01, 'adam_eps': 1e-06, 'dropout': 0.1, 'alpha': 2, 'train': 1, 'backbone': 't5-small', 'metrics': 'hit@5,hit@10,ndcg@5,ndcg@10', 'load': 0, 'random_initialize': 1, 'test_epoch': 0, 'valid_select': 0, 'test_before_train': 0, 'test_filtered': 0, 'test_filtered_batch': 1, 'world_size': 2, 'rank': 0, 'log_name': '1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt', 'model_path': '../model/ML1M/1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt.pt'}
2025-03-07 03:17:04,713 - root - INFO - Generating data for ML1M dataset
2025-03-07 03:17:04,713 - root - INFO - Get prompt template from ../prompt.txt
2025-03-07 03:18:09,114 - root - INFO - {'seed': 2023, 'model_dir': '../model', 'checkpoint_dir': '../checkpoint', 'model_name': 'model.pt', 'log_dir': '../log', 'distributed': 1, 'gpu': '2,3', 'master_addr': 'localhost', 'master_port': '1984', 'logging_level': 20, 'data_path': '../data', 'item_indexing': 'sequential', 'tasks': 'sequential,straightforward', 'datasets': 'ML1M', 'prompt_file': '../prompt.txt', 'sequential_order': 'original', 'collaborative_token_size': 200, 'collaborative_cluster': 20, 'collaborative_last_token': 'sequential', 'collaborative_float32': 0, 'max_his': 20, 'his_prefix': 1, 'his_sep': ' , ', 'skip_empty_his': 1, 'valid_prompt': 'seen:0', 'valid_prompt_sample': 1, 'valid_sample_num': '3,3', 'test_prompt': 'seen:0', 'sample_prompt': 1, 'sample_num': '3,3', 'batch_size': 128, 'eval_batch_size': 20, 'dist_sampler': 0, 'optim': 'AdamW', 'epochs': 10, 'lr': 0.001, 'clip': 1, 'logging_step': 100, 'warmup_prop': 0.05, 'gradient_accumulation_steps': 1, 'weight_decay': 0.01, 'adam_eps': 1e-06, 'dropout': 0.1, 'alpha': 2, 'train': 1, 'backbone': 't5-small', 'metrics': 'hit@5,hit@10,ndcg@5,ndcg@10', 'load': 0, 'random_initialize': 1, 'test_epoch': 0, 'valid_select': 0, 'test_before_train': 0, 'test_filtered': 0, 'test_filtered_batch': 1, 'world_size': 2, 'rank': 0, 'log_name': '1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt', 'model_path': '../model/ML1M/1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt.pt'}
2025-03-07 03:18:09,114 - root - INFO - Generating data for ML1M dataset
2025-03-07 03:18:09,114 - root - INFO - Get prompt template from ../prompt.txt
2025-03-07 03:18:09,118 - root - INFO - Considering {dataset} user_{user_id} has interacted with {dataset} items {history} . What is the next recommendation for the user ?
2025-03-07 03:18:09,118 - root - INFO - Required info: ['dataset', 'user_id', 'target', 'history']
2025-03-07 03:26:33,828 - root - INFO - {'seed': 2023, 'model_dir': '../model', 'checkpoint_dir': '../checkpoint', 'model_name': 'model.pt', 'log_dir': '../log', 'distributed': 1, 'gpu': '2,3', 'master_addr': 'localhost', 'master_port': '1984', 'logging_level': 20, 'data_path': '../data', 'item_indexing': 'sequential', 'tasks': 'sequential,straightforward', 'datasets': 'ML1M', 'prompt_file': '../prompt.txt', 'sequential_order': 'original', 'collaborative_token_size': 200, 'collaborative_cluster': 20, 'collaborative_last_token': 'sequential', 'collaborative_float32': 0, 'max_his': 20, 'his_prefix': 1, 'his_sep': ' , ', 'skip_empty_his': 1, 'valid_prompt': 'seen:0', 'valid_prompt_sample': 1, 'valid_sample_num': '3,3', 'test_prompt': 'seen:0', 'sample_prompt': 1, 'sample_num': '3,3', 'batch_size': 128, 'eval_batch_size': 20, 'dist_sampler': 0, 'optim': 'AdamW', 'epochs': 10, 'lr': 0.001, 'clip': 1, 'logging_step': 100, 'warmup_prop': 0.05, 'gradient_accumulation_steps': 1, 'weight_decay': 0.01, 'adam_eps': 1e-06, 'dropout': 0.1, 'alpha': 2, 'train': 1, 'backbone': 't5-small', 'metrics': 'hit@5,hit@10,ndcg@5,ndcg@10', 'load': 0, 'random_initialize': 1, 'test_epoch': 0, 'valid_select': 0, 'test_before_train': 0, 'test_filtered': 0, 'test_filtered_batch': 1, 'world_size': 2, 'rank': 0, 'log_name': '1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt', 'model_path': '../model/ML1M/1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt.pt'}
2025-03-07 03:26:33,829 - root - INFO - Generating data for ML1M dataset
2025-03-07 03:26:33,829 - root - INFO - Get prompt template from ../prompt.txt
2025-03-07 03:26:33,830 - root - INFO - Considering {dataset} user_{user_id} has interacted with {dataset} items {history} . What is the next recommendation for the user ?
2025-03-07 03:26:33,830 - root - INFO - Required info: ['user_id', 'history', 'dataset', 'target']
2025-03-07 03:30:58,742 - root - INFO - {'seed': 2023, 'model_dir': '../model', 'checkpoint_dir': '../checkpoint', 'model_name': 'model.pt', 'log_dir': '../log', 'distributed': 1, 'gpu': '2,3', 'master_addr': 'localhost', 'master_port': '1984', 'logging_level': 20, 'data_path': '../data', 'item_indexing': 'sequential', 'tasks': 'sequential,straightforward', 'datasets': 'ML1M', 'prompt_file': '../prompt.txt', 'sequential_order': 'original', 'collaborative_token_size': 200, 'collaborative_cluster': 20, 'collaborative_last_token': 'sequential', 'collaborative_float32': 0, 'max_his': 20, 'his_prefix': 1, 'his_sep': ' , ', 'skip_empty_his': 1, 'valid_prompt': 'seen:0', 'valid_prompt_sample': 1, 'valid_sample_num': '3,3', 'test_prompt': 'seen:0', 'sample_prompt': 1, 'sample_num': '3,3', 'batch_size': 128, 'eval_batch_size': 20, 'dist_sampler': 0, 'optim': 'AdamW', 'epochs': 10, 'lr': 0.001, 'clip': 1, 'logging_step': 100, 'warmup_prop': 0.05, 'gradient_accumulation_steps': 1, 'weight_decay': 0.01, 'adam_eps': 1e-06, 'dropout': 0.1, 'alpha': 2, 'train': 1, 'backbone': 't5-small', 'metrics': 'hit@5,hit@10,ndcg@5,ndcg@10', 'load': 0, 'random_initialize': 1, 'test_epoch': 0, 'valid_select': 0, 'test_before_train': 0, 'test_filtered': 0, 'test_filtered_batch': 1, 'world_size': 2, 'rank': 0, 'log_name': '1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt', 'model_path': '../model/ML1M/1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt.pt'}
2025-03-07 03:30:58,742 - root - INFO - Generating data for ML1M dataset
2025-03-07 03:30:58,742 - root - INFO - Get prompt template from ../prompt.txt
2025-03-07 03:30:58,744 - root - INFO - Considering {dataset} user_{user_id} has interacted with {dataset} items {history} . What is the next recommendation for the user ?
2025-03-07 03:30:58,744 - root - INFO - Required info: ['user_id', 'target', 'dataset', 'history']
2025-03-07 03:37:23,814 - root - INFO - {'seed': 2023, 'model_dir': '../model', 'checkpoint_dir': '../checkpoint', 'model_name': 'model.pt', 'log_dir': '../log', 'distributed': 1, 'gpu': '2,3', 'master_addr': 'localhost', 'master_port': '1984', 'logging_level': 20, 'data_path': '../data', 'item_indexing': 'sequential', 'tasks': 'sequential,straightforward', 'datasets': 'ML1M', 'prompt_file': '../prompt.txt', 'sequential_order': 'original', 'collaborative_token_size': 200, 'collaborative_cluster': 20, 'collaborative_last_token': 'sequential', 'collaborative_float32': 0, 'max_his': 20, 'his_prefix': 1, 'his_sep': ' , ', 'skip_empty_his': 1, 'valid_prompt': 'seen:0', 'valid_prompt_sample': 1, 'valid_sample_num': '3,3', 'test_prompt': 'seen:0', 'sample_prompt': 1, 'sample_num': '3,3', 'batch_size': 128, 'eval_batch_size': 20, 'dist_sampler': 0, 'optim': 'AdamW', 'epochs': 10, 'lr': 0.001, 'clip': 1, 'logging_step': 100, 'warmup_prop': 0.05, 'gradient_accumulation_steps': 1, 'weight_decay': 0.01, 'adam_eps': 1e-06, 'dropout': 0.1, 'alpha': 2, 'train': 1, 'backbone': 't5-small', 'metrics': 'hit@5,hit@10,ndcg@5,ndcg@10', 'load': 0, 'random_initialize': 1, 'test_epoch': 0, 'valid_select': 0, 'test_before_train': 0, 'test_filtered': 0, 'test_filtered_batch': 1, 'world_size': 2, 'rank': 0, 'log_name': '1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt', 'model_path': '../model/ML1M/1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt.pt'}
2025-03-07 03:37:23,814 - root - INFO - Generating data for ML1M dataset
2025-03-07 03:37:23,814 - root - INFO - Get prompt template from ../prompt.txt
2025-03-07 03:37:23,815 - root - INFO - Considering {dataset} user_{user_id} has interacted with {dataset} items {history} . What is the next recommendation for the user ?
2025-03-07 03:37:23,815 - root - INFO - Required info: ['user_id', 'dataset', 'target', 'history']
2025-03-07 03:37:23,935 - root - INFO - Reindex data with sequential indexing method
2025-03-07 03:40:09,800 - root - INFO - {'seed': 2023, 'model_dir': '../model', 'checkpoint_dir': '../checkpoint', 'model_name': 'model.pt', 'log_dir': '../log', 'distributed': 1, 'gpu': '2,3', 'master_addr': 'localhost', 'master_port': '1984', 'logging_level': 20, 'data_path': '../data', 'item_indexing': 'sequential', 'tasks': 'sequential,straightforward', 'datasets': 'ML1M', 'prompt_file': '../prompt.txt', 'sequential_order': 'original', 'collaborative_token_size': 200, 'collaborative_cluster': 20, 'collaborative_last_token': 'sequential', 'collaborative_float32': 0, 'max_his': 20, 'his_prefix': 1, 'his_sep': ' , ', 'skip_empty_his': 1, 'valid_prompt': 'seen:0', 'valid_prompt_sample': 1, 'valid_sample_num': '3,3', 'test_prompt': 'seen:0', 'sample_prompt': 1, 'sample_num': '3,3', 'batch_size': 128, 'eval_batch_size': 20, 'dist_sampler': 0, 'optim': 'AdamW', 'epochs': 10, 'lr': 0.001, 'clip': 1, 'logging_step': 100, 'warmup_prop': 0.05, 'gradient_accumulation_steps': 1, 'weight_decay': 0.01, 'adam_eps': 1e-06, 'dropout': 0.1, 'alpha': 2, 'train': 1, 'backbone': 't5-small', 'metrics': 'hit@5,hit@10,ndcg@5,ndcg@10', 'load': 0, 'random_initialize': 1, 'test_epoch': 0, 'valid_select': 0, 'test_before_train': 0, 'test_filtered': 0, 'test_filtered_batch': 1, 'world_size': 2, 'rank': 0, 'log_name': '1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt', 'model_path': '../model/ML1M/1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt.pt'}
2025-03-07 03:40:09,801 - root - INFO - Generating data for ML1M dataset
2025-03-07 03:40:09,801 - root - INFO - Get prompt template from ../prompt.txt
2025-03-07 03:40:09,801 - root - INFO - Considering {dataset} user_{user_id} has interacted with {dataset} items {history} . What is the next recommendation for the user ?
2025-03-07 03:40:09,802 - root - INFO - Required info: ['dataset', 'user_id', 'target', 'history']
2025-03-07 03:40:09,909 - root - INFO - Reindex data with sequential indexing method
2025-03-07 03:40:26,703 - root - INFO - loading training data
2025-03-07 03:40:31,877 - root - INFO - Getting prompt information
2025-03-07 03:41:08,608 - root - INFO - Input: Here is the purchase history of ML1M user_1 : ML1M item item_1015 , item_1016 , item_1017 , item_1018 , item_1019 , item_1020 , item_1021 , item_1022 , item_1023 , item_1024 , item_1025 , item_1026 , item_1027 , item_1028 , item_1029 , item_1030 , item_1031 , item_1032 , item_1033 , item_1034 . I wonder what is the next recommended item for the user . , Output: ML1M item_1035 
2025-03-07 03:41:09,023 - root - INFO - Use t5-small backbone model
2025-03-07 03:41:12,862 - root - INFO - Random initialize number related tokens
2025-03-07 03:43:27,848 - root - INFO - {'seed': 2023, 'model_dir': '../model', 'checkpoint_dir': '../checkpoint', 'model_name': 'model.pt', 'log_dir': '../log', 'distributed': 1, 'gpu': '2,3', 'master_addr': 'localhost', 'master_port': '1984', 'logging_level': 20, 'data_path': '../data', 'item_indexing': 'sequential', 'tasks': 'sequential,straightforward', 'datasets': 'ML1M', 'prompt_file': '../prompt.txt', 'sequential_order': 'original', 'collaborative_token_size': 200, 'collaborative_cluster': 20, 'collaborative_last_token': 'sequential', 'collaborative_float32': 0, 'max_his': 20, 'his_prefix': 1, 'his_sep': ' , ', 'skip_empty_his': 1, 'valid_prompt': 'seen:0', 'valid_prompt_sample': 1, 'valid_sample_num': '3,3', 'test_prompt': 'seen:0', 'sample_prompt': 1, 'sample_num': '3,3', 'batch_size': 128, 'eval_batch_size': 20, 'dist_sampler': 0, 'optim': 'AdamW', 'epochs': 10, 'lr': 0.001, 'clip': 1, 'logging_step': 100, 'warmup_prop': 0.05, 'gradient_accumulation_steps': 1, 'weight_decay': 0.01, 'adam_eps': 1e-06, 'dropout': 0.1, 'alpha': 2, 'train': 1, 'backbone': 't5-small', 'metrics': 'hit@5,hit@10,ndcg@5,ndcg@10', 'load': 0, 'random_initialize': 1, 'test_epoch': 0, 'valid_select': 0, 'test_before_train': 0, 'test_filtered': 0, 'test_filtered_batch': 1, 'world_size': 2, 'rank': 0, 'log_name': '1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt', 'model_path': '../model/ML1M/1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt.pt'}
2025-03-07 03:43:27,849 - root - INFO - Generating data for ML1M dataset
2025-03-07 03:43:27,849 - root - INFO - Get prompt template from ../prompt.txt
2025-03-07 03:43:27,849 - root - INFO - Considering {dataset} user_{user_id} has interacted with {dataset} items {history} . What is the next recommendation for the user ?
2025-03-07 03:43:27,850 - root - INFO - Required info: ['history', 'dataset', 'user_id', 'target']
2025-03-07 03:43:27,957 - root - INFO - Reindex data with sequential indexing method
2025-03-07 03:43:33,500 - root - INFO - loading training data
2025-03-07 03:43:38,440 - root - INFO - Getting prompt information
2025-03-07 03:44:13,494 - root - INFO - Input: Here is the purchase history of ML1M user_1 : ML1M item item_1015 , item_1016 , item_1017 , item_1018 , item_1019 , item_1020 , item_1021 , item_1022 , item_1023 , item_1024 , item_1025 , item_1026 , item_1027 , item_1028 , item_1029 , item_1030 , item_1031 , item_1032 , item_1033 , item_1034 . I wonder what is the next recommended item for the user . , Output: ML1M item_1035 
2025-03-07 03:44:13,989 - root - INFO - Use t5-small backbone model
2025-03-07 03:44:15,428 - root - INFO - Random initialize number related tokens
2025-03-07 03:44:18,261 - root - INFO - Building Optimizer and Scheduler
2025-03-07 03:44:18,261 - root - INFO - Batch per epoch: 23004
2025-03-07 03:44:18,262 - root - INFO - Total steps: 230040
2025-03-07 03:44:18,262 - root - INFO - Warmup proportion: 0.05
2025-03-07 03:44:18,262 - root - INFO - Warm up steps: 11502
2025-03-07 03:44:18,264 - root - INFO - Building Optimizer AdamW
2025-03-07 03:44:18,767 - root - INFO - Start training
2025-03-07 03:44:18,768 - root - INFO - Start training for epoch 1
2025-03-07 03:44:55,064 - root - INFO - Input: According to what items ML1M user_1 has purchased : ML1M items item_1015 , item_1016 , item_1017 , item_1018 , item_1019 , item_1020 , item_1021 , item_1022 , item_1023 , item_1024 , item_1025 , item_1026 , item_1027 , item_1028 , item_1029 , item_1030 , item_1031 , item_1032 , item_1033 , item_1034 , Can you recommend another item to the user ? , Output: ML1M item_1035 
2025-03-07 05:26:47,237 - root - INFO - The average training loss for epoch 1 is 0.9282523393630981
2025-03-07 05:26:47,240 - root - INFO - Start training for epoch 2
2025-03-07 05:27:22,559 - root - INFO - Input: After buying ML1M items item_1015 , item_1016 , item_1017 , item_1018 , item_1019 , item_1020 , item_1021 , item_1022 , item_1023 , item_1024 , item_1025 , item_1026 , item_1027 , item_1028 , item_1029 , item_1030 , item_1031 , item_1032 , item_1033 , item_1034 , what is the next item that could be recommended for ML1M user_1 ? , Output: ML1M item_1035 
2025-03-07 07:09:13,465 - root - INFO - The average training loss for epoch 2 is 0.7607194781303406
2025-03-07 07:09:13,469 - root - INFO - Start training for epoch 3
2025-03-07 07:09:49,862 - root - INFO - Input: According to what items ML1M user_1 has purchased : ML1M items item_1015 , item_1016 , item_1017 , item_1018 , item_1019 , item_1020 , item_1021 , item_1022 , item_1023 , item_1024 , item_1025 , item_1026 , item_1027 , item_1028 , item_1029 , item_1030 , item_1031 , item_1032 , item_1033 , item_1034 , Can you recommend another item to the user ? , Output: ML1M item_1035 
2025-03-07 08:51:41,154 - root - INFO - The average training loss for epoch 3 is 0.737473726272583
2025-03-07 08:51:41,158 - root - INFO - Start training for epoch 4
2025-03-07 08:52:16,831 - root - INFO - Input: Here is the purchase history of ML1M user_1 : ML1M item item_1015 , item_1016 , item_1017 , item_1018 , item_1019 , item_1020 , item_1021 , item_1022 , item_1023 , item_1024 , item_1025 , item_1026 , item_1027 , item_1028 , item_1029 , item_1030 , item_1031 , item_1032 , item_1033 , item_1034 . I wonder what is the next recommended item for the user . , Output: ML1M item_1035 
2025-03-07 10:34:10,361 - root - INFO - The average training loss for epoch 4 is 0.7230737805366516
2025-03-07 10:34:10,365 - root - INFO - Start training for epoch 5
2025-03-07 10:34:45,907 - root - INFO - Input: What would ML1M user_1 be likely to purchase next after buying ML1M items item_1015 , item_1016 , item_1017 , item_1018 , item_1019 , item_1020 , item_1021 , item_1022 , item_1023 , item_1024 , item_1025 , item_1026 , item_1027 , item_1028 , item_1029 , item_1030 , item_1031 , item_1032 , item_1033 , item_1034 ? , Output: ML1M item_1035 
2025-03-07 12:16:38,291 - root - INFO - The average training loss for epoch 5 is 0.7124285697937012
2025-03-07 12:16:38,296 - root - INFO - Start training for epoch 6
2025-03-07 12:17:14,271 - root - INFO - Input: The ML1M user_1 has bought items : ML1M items item_1015 , item_1016 , item_1017 , item_1018 , item_1019 , item_1020 , item_1021 , item_1022 , item_1023 , item_1024 , item_1025 , item_1026 , item_1027 , item_1028 , item_1029 , item_1030 , item_1031 , item_1032 , item_1033 , item_1034 , What else do you think is necessary for the user ? , Output: ML1M item_1035 
2025-03-07 13:59:09,034 - root - INFO - The average training loss for epoch 6 is 0.7036458849906921
2025-03-07 13:59:09,039 - root - INFO - Start training for epoch 7
2025-03-07 13:59:44,463 - root - INFO - Input: Can you recommend the next item for ML1M user_1 , given the user 's purchase of ML1M items item_1015 , item_1016 , item_1017 , item_1018 , item_1019 , item_1020 , item_1021 , item_1022 , item_1023 , item_1024 , item_1025 , item_1026 , item_1027 , item_1028 , item_1029 , item_1030 , item_1031 , item_1032 , item_1033 , item_1034 ? , Output: ML1M item_1035 
2025-03-07 15:41:38,932 - root - INFO - The average training loss for epoch 7 is 0.6957178115844727
2025-03-07 15:41:38,938 - root - INFO - Start training for epoch 8
2025-03-07 15:42:15,100 - root - INFO - Input: Here is the purchase history of ML1M user_1 : ML1M item item_1015 , item_1016 , item_1017 , item_1018 , item_1019 , item_1020 , item_1021 , item_1022 , item_1023 , item_1024 , item_1025 , item_1026 , item_1027 , item_1028 , item_1029 , item_1030 , item_1031 , item_1032 , item_1033 , item_1034 . I wonder what is the next recommended item for the user . , Output: ML1M item_1035 
2025-03-07 17:24:09,021 - root - INFO - The average training loss for epoch 8 is 0.6882278323173523
2025-03-07 17:24:09,026 - root - INFO - Start training for epoch 9
2025-03-07 17:24:45,233 - root - INFO - Input: What would ML1M user_1 be likely to purchase next after buying ML1M items item_1015 , item_1016 , item_1017 , item_1018 , item_1019 , item_1020 , item_1021 , item_1022 , item_1023 , item_1024 , item_1025 , item_1026 , item_1027 , item_1028 , item_1029 , item_1030 , item_1031 , item_1032 , item_1033 , item_1034 ? , Output: ML1M item_1035 
2025-03-07 19:06:40,133 - root - INFO - The average training loss for epoch 9 is 0.681047797203064
2025-03-07 19:06:40,137 - root - INFO - Start training for epoch 10
2025-03-07 19:07:15,853 - root - INFO - Input: By analyzing the ML1M user_1 's purchase of ML1M items item_1015 , item_1016 , item_1017 , item_1018 , item_1019 , item_1020 , item_1021 , item_1022 , item_1023 , item_1024 , item_1025 , item_1026 , item_1027 , item_1028 , item_1029 , item_1030 , item_1031 , item_1032 , item_1033 , item_1034 , what is the next item expected to be bought ? , Output: ML1M item_1035 
2025-03-07 20:49:12,577 - root - INFO - The average training loss for epoch 10 is 0.6748380661010742
2025-03-07 20:49:13,214 - root - INFO - Save the current model to ../model/ML1M/1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt.pt
2025-03-07 20:49:13,313 - root - INFO - Load model from ../model/ML1M/1_1_1_1_20_1984_ML1M_sequential,straightforward_t5-small_sequential_0.001_10_128_3,3_prompt.pt
2025-03-07 20:49:13,570 - root - INFO - testing ML1M dataset on sequential task
2025-03-07 20:51:35,519 - root - INFO - hit@5: 0.16804635761589404
2025-03-07 20:51:35,519 - root - INFO - hit@10: 0.24933774834437086
2025-03-07 20:51:35,520 - root - INFO - ndcg@5: 0.11335127690045252
2025-03-07 20:51:35,520 - root - INFO - ndcg@10: 0.13947079922986194
2025-03-07 20:51:35,520 - root - INFO - testing ML1M dataset on straightforward task
2025-03-07 20:53:46,560 - root - INFO - hit@5: 0.0097682119205298
2025-03-07 20:53:46,560 - root - INFO - hit@10: 0.021357615894039735
2025-03-07 20:53:46,560 - root - INFO - ndcg@5: 0.005304589217363546
2025-03-07 20:53:46,560 - root - INFO - ndcg@10: 0.008967165331522226
